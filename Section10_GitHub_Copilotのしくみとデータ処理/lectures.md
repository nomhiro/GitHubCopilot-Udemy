# GitHub Copilotのしくみとデータ処理（15%）

## 参照リソース
- [GitHub Copilot ユーザー プロンプト プロセス フロー - Microsoft Learn](https://learn.microsoft.com/ja-jp/training/modules/introduction-prompt-engineering-with-github-copilot/3-github-copilot-user-prompt-process-flow)
- [GitHub Copilot データ - Microsoft Learn](https://learn.microsoft.com/ja-jp/training/modules/introduction-prompt-engineering-with-github-copilot/4-github-copilot-data)
- [GitHub Copilot 大規模言語モデル (LLM) - Microsoft Learn](https://learn.microsoft.com/ja-jp/training/modules/introduction-prompt-engineering-with-github-copilot/5-github-copilot-large-language-models)
- [GitHub Copilot での契約上の保護と一致するパブリック コードの無効化 - Microsoft Learn](https://learn.microsoft.com/ja-jp/training/modules/github-copilot-management-and-customizations/3-github-copilot-contractual-protections-disabling-matching-public-code)

---

## レクチャー1: コード提案のデータパイプラインライフサイクル
**時間目安: 12分**

### 学習目標
- GitHub Copilotのデータフロー全体像を理解する
- インバウンドフローとアウトバウンドフローの違いを説明できる
- 7つの処理ステップを順序立てて説明できる

### 内容

#### GitHub Copilotの処理フロー概要

GitHub Copilotがプロンプトを受け取り、コード提案を返すまでには、**インバウンドフロー**と**アウトバウンドフロー**の2つの流れがあります。

```
┌─────────────────────────────────────────────────────────────────┐
│                    インバウンドフロー                            │
│  ユーザー → コンテキスト収集 → プロキシフィルター → LLM          │
├─────────────────────────────────────────────────────────────────┤
│                    アウトバウンドフロー                          │
│  LLM → 後処理・検証 → フィルタリング → ユーザーへ提案           │
└─────────────────────────────────────────────────────────────────┘
```

#### 7つの処理ステップ

| ステップ | フロー | 処理内容 |
|---------|--------|----------|
| 1 | インバウンド | 安全なプロンプト送信とコンテキスト収集 |
| 2 | インバウンド | プロキシフィルター（不正なプロンプト操作の防止） |
| 3 | インバウンド | トキシシティフィルタリング（有害コンテンツの除外） |
| 4 | インバウンド | LLMによるコード生成 |
| 5 | アウトバウンド | 後処理とレスポンス検証 |
| 6 | アウトバウンド | 提案のデリバリーとフィードバックループ開始 |
| 7 | アウトバウンド | 後続のプロンプトに対するプロセスの繰り返し |

#### インバウンドフローの役割

インバウンドフローでは、ユーザーの入力を安全にLLMに届けるための処理が行われます：

1. **セキュアな通信**: HTTPSによる暗号化通信
2. **コンテキストの構築**: 周囲のコードや開いているファイルからの情報収集
3. **フィルタリング**: 悪意のあるプロンプトや不適切なコンテンツの除外
4. **LLMへの送信**: 整形されたプロンプトをモデルに渡す

#### アウトバウンドフローの役割

アウトバウンドフローでは、LLMからの応答を安全にユーザーに届けるための処理が行われます：

1. **品質チェック**: コードの脆弱性やバグのチェック
2. **パブリックコードとの照合**: 公開コードとの一致確認（オプション）
3. **フィルタリング**: 不適切なコンテンツの除外
4. **提案の表示**: ユーザーへのコード提案の提示
5. **フィードバック収集**: ユーザーの操作（承認/拒否/編集）から学習

> **重要**: このプロセス全体は数ミリ秒から数秒で完了し、リアルタイムでコード提案が行われます。

---

## レクチャー2: コンテキスト収集とプロンプト構築の仕組み
**時間目安: 12分**

### 学習目標
- コンテキスト収集の仕組みを理解する
- Fill-in-the-Middle（FIM）手法を説明できる
- 効果的なコンテキスト提供の方法を把握する

### 内容

#### 安全なプロンプト送信

GitHub Copilotへのプロンプト送信は、以下のセキュリティ対策が施されています：

- **HTTPS通信**: すべてのプロンプトはHTTPSで暗号化されて送信
- **機密性の保護**: 送信されたコードやコメントは安全に保護
- **認証**: GitHubアカウントによる認証が必要

#### コンテキスト収集の詳細

GitHub Copilotは、以下の情報を収集してプロンプトを構築します：

| 収集する情報 | 説明 |
|-------------|------|
| **カーソル前のコード** | 現在編集中の行より前のコード（プレフィックス） |
| **カーソル後のコード** | 現在編集中の行より後のコード（サフィックス） |
| **ファイル名とタイプ** | 編集中のファイルの名前と拡張子（言語の特定に使用） |
| **隣接する開いているタブ** | 同じプロジェクト内で開いている他のファイル |
| **プロジェクト構造** | ファイルパスやディレクトリ構造 |
| **プログラミング言語** | 使用されている言語とフレームワーク |

#### Fill-in-the-Middle（FIM）手法

FIM手法は、GitHub Copilotの精度を大幅に向上させる重要な技術です。

```
┌─────────────────────────────────────────────────────────────┐
│  プレフィックス（カーソル前のコード）                        │
│  ────────────────────────────────                           │
│  def calculate_tax(price, rate):                            │
│      """税額を計算する関数"""                                │
│      ┌─────────────────────────┐                            │
│      │ ← ここにカーソル         │ ← Copilotが提案           │
│      └─────────────────────────┘                            │
│  ────────────────────────────────                           │
│  サフィックス（カーソル後のコード）                          │
│      return result                                          │
└─────────────────────────────────────────────────────────────┘
```

**FIMの利点**:
- カーソルの**前後両方**のコンテキストを使用
- 既存のコード構造に**適合する**提案が可能
- より**正確で関連性の高い**コード生成

#### コンテキストの優先順位

Copilotは以下の優先順位でコンテキストを処理します：

1. **現在のファイル**: 最も重要なコンテキスト
2. **開いているタブ**: 関連性の高いファイル
3. **プロジェクト構造**: ファイルパスやディレクトリ名
4. **言語とフレームワーク**: 構文やライブラリの推測

> **ヒント**: より良い提案を得るには、関連するファイルをタブで開いておくと効果的です。

---

## レクチャー3: プロキシサービスとフィルター処理
**時間目安: 10分**

### 学習目標
- プロキシサービスの役割を理解する
- トキシシティフィルタリングの目的を説明できる
- フィルター処理の種類を把握する

### 内容

#### プロキシサービスの役割

GitHub Copilotのプロキシサーバーは、**GitHubが所有するMicrosoft Azureテナント**でホストされており、以下の重要な役割を果たします：

```
┌──────────┐      ┌──────────────────┐      ┌─────────┐
│ ユーザー │ ───→ │ プロキシサーバー │ ───→ │   LLM   │
│   IDE    │      │ (Azure テナント)  │      │ (OpenAI) │
└──────────┘      └──────────────────┘      └─────────┘
                         │
                    ┌────┴────┐
                    │フィルター│
                    │  処理   │
                    └─────────┘
```

**プロキシサーバーの機能**:
| 機能 | 説明 |
|------|------|
| **トラフィックフィルタリング** | 不正なプロンプト操作の試みをブロック |
| **プロンプトハッキング防止** | モデルの内部動作を暴露させる攻撃を防止 |
| **リクエスト転送** | フィルタリング後のプロンプトをLLMに転送 |
| **レスポンス検証** | LLMからの応答を検証して返送 |

#### トキシシティフィルタリング

コード生成の前に、以下の有害コンテンツを除外するフィルタリングが適用されます：

**1. ヘイトスピーチと不適切なコンテンツの検出**
- 攻撃的な言語やヘイトスピーチを検出
- 不適切または有害なコンテンツを除外
- アルゴリズムによる自動検出

**2. 個人データの保護**
- 名前、住所、識別番号などの個人情報をフィルタリング
- ユーザーのプライバシーとデータセキュリティを保護
- 機密情報の漏洩を防止

#### フィルター処理の流れ

```
入力プロンプト
      │
      ▼
┌─────────────────┐
│ プロキシフィルター │ ← 不正操作の防止
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│トキシシティフィルター│ ← 有害コンテンツの除外
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│ 個人データフィルター │ ← プライバシー保護
└────────┬────────┘
         │
         ▼
    LLMへ送信
```

> **注意**: フィルター処理はGitHub Copilot for BusinessおよびEnterpriseでも同様に適用されます。ユーザーのコードがモデルのトレーニングに使用されることはありません。

---

## レクチャー4: 大規模言語モデル（LLM）のレスポンス生成
**時間目安: 10分**

### 学習目標
- LLM（大規模言語モデル）の基本概念を理解する
- GitHub CopilotにおけるLLMの役割を説明できる
- ファインチューニングとLoRAの概念を把握する

### 内容

#### 大規模言語モデル（LLM）とは

LLM（Large Language Models）は、人間の言語を理解、生成、操作するために設計されたAIモデルです。

| 特性 | 説明 |
|------|------|
| **大量のトレーニングデータ** | 多様なソースから膨大なテキストデータで学習 |
| **コンテキスト理解** | 文脈に適した、意味のあるテキストを生成 |
| **機械学習とAI** | 数百万から数十億のパラメータを持つニューラルネットワーク |
| **汎用性** | 特定のタスクやドメインに適応可能 |

#### GitHub CopilotでのLLMの役割

GitHub Copilotは、LLMを使用してコンテキストを認識したコード提案を行います：

```
┌───────────────────────────────────────────────────────────┐
│                    LLMの処理フロー                        │
├───────────────────────────────────────────────────────────┤
│  1. プロンプト受信 → 入力トークンに分割                   │
│  2. コンテキスト分析 → 周囲のコードパターンを理解         │
│  3. コード生成 → 最も確率の高い次のトークンを予測         │
│  4. 出力生成 → 完全なコード提案を構築                     │
└───────────────────────────────────────────────────────────┘
```

**LLMが考慮する要素**:
- 現在のファイルのコード
- 開いている他のファイルやタブ
- プロジェクト全体の構造
- プログラミング言語の構文

#### ファインチューニング

ファインチューニングは、事前トレーニング済みのLLMを特定のタスクやドメインに適応させるプロセスです。

```
┌─────────────────┐      ┌─────────────────┐      ┌─────────────────┐
│ ソースモデル     │      │ ターゲット       │      │ ファインチューン │
│ (大規模な事前    │ ───→ │ データセット     │ ───→ │ されたモデル     │
│  トレーニング済) │      │ (タスク固有)     │      │ (特化型)        │
└─────────────────┘      └─────────────────┘      └─────────────────┘
```

#### LoRA（Low-Rank Adaptation）ファインチューニング

GitHubは、効率的なファインチューニング手法である**LoRA**を使用しています。

**従来のファインチューニング**:
- ニューラルネットワークのすべてのパラメータを再トレーニング
- 時間とリソースを大量に消費

**LoRAファインチューニング**:
- 各レイヤーに小さなトレーニング可能なパラメータを追加
- オリジナルモデルは変更せず保持
- 時間とリソースを大幅に節約

| 比較項目 | 従来のファインチューニング | LoRA |
|---------|-------------------------|------|
| トレーニング対象 | 全パラメータ | 追加パラメータのみ |
| リソース消費 | 高い | 低い |
| 処理時間 | 長い | 短い |
| 性能 | 良好 | 同等以上 |

> **ポイント**: LoRAは「より賢く働く、より懸命に働かない」アプローチで、効率的にLLMを特定の要件に適応させます。

---

## レクチャー5: 一致コード識別と後処理
**時間目安: 10分**

### 学習目標
- 後処理とレスポンス検証の仕組みを理解する
- 一致するパブリックコードのフィルタリングを説明できる
- 契約上の保護（IP補償）について把握する

### 内容

#### 後処理とレスポンス検証

LLMがレスポンスを生成した後、GitHub Copilotは以下の後処理を行います：

```
┌────────────────────────────────────────────────────────────┐
│                    後処理フロー                            │
├────────────────────────────────────────────────────────────┤
│  LLMレスポンス                                             │
│       │                                                    │
│       ▼                                                    │
│  ┌──────────────────┐                                      │
│  │ トキシシティフィルター│ ← 有害コンテンツの除去           │
│  └────────┬─────────┘                                      │
│           │                                                │
│           ▼                                                │
│  ┌──────────────────┐                                      │
│  │ コード品質チェック │ ← バグ・脆弱性の検出                │
│  └────────┬─────────┘                                      │
│           │                                                │
│           ▼                                                │
│  ┌──────────────────┐                                      │
│  │パブリックコード照合│ ← 一致コードのフィルタリング        │
│  └────────┬─────────┘                                      │
│           │                                                │
│           ▼                                                │
│     ユーザーに提案                                         │
└────────────────────────────────────────────────────────────┘
```

#### コード品質チェック

提案されるコードは、以下のセキュリティチェックが行われます：

| チェック項目 | 説明 |
|-------------|------|
| **XSS（クロスサイトスクリプティング）** | Webアプリケーションの脆弱性検出 |
| **SQLインジェクション** | データベース攻撃の脆弱性検出 |
| **一般的なバグ** | よくあるプログラミングエラーの検出 |
| **セキュリティベストプラクティス** | セキュアコーディング規約の確認 |

#### 一致するパブリックコードのフィルタリング

管理者は、**パブリックコードと一致する提案をブロック**するフィルターを有効にできます：

**フィルターの動作**:
- 約**150文字以上**の提案が対象
- GitHubの既存パブリックコードと照合
- 一致が検出された場合、提案は切り詰めまたは破棄

**設定方法**:
1. GitHubでプロフィール写真を選択
2. **Your enterprises** または **Your organizations** を選択
3. **Settings** → **Copilot** に移動
4. **Suggestions** で **Matching public code** を **Block** に設定

#### 契約上の保護（IP補償）

GitHub Copilot BusinessおよびEnterpriseプランでは、**知的財産（IP）補償**が提供されます：

| 保護の種類 | 説明 |
|-----------|------|
| **IP補償** | Copilotの提案が第三者のIP権を侵害していると訴えられた場合、GitHubが法的責任を負う |
| **データ保護契約（DPA）** | データ保護とプライバシー規制への準拠を保証 |
| **GitHub Copilot Trust Center** | セキュリティ、プライバシー、コンプライアンスに関する詳細情報を提供 |

> **重要**: IP補償を受けるためには、**Matching public code** 設定を **Block** に設定する必要があります。

#### フィードバックループ

提案がユーザーに表示された後、フィードバックループが開始されます：

- **承認された提案**: 知識ベースの強化に活用
- **編集された提案**: 改善点の学習に活用
- **拒否された提案**: 不適切なパターンの特定に活用

---

## レクチャー6: データフロー（Individual/Chat/コード補完）
**時間目安: 12分**

### 学習目標
- プラン別のデータ処理の違いを理解する
- コード補完とCopilot Chatのデータ保持の違いを説明できる
- プライバシー設定のオプションを把握する

### 内容

#### コード補完のデータ処理

GitHub Copilotのコード補完機能では、**プロンプトや提案は保持されません**：

```
┌─────────────────────────────────────────────────────────────┐
│                コード補完のデータフロー                      │
├─────────────────────────────────────────────────────────────┤
│  1. ユーザーがコードを入力                                   │
│  2. コンテキストとプロンプトがLLMに送信                      │
│  3. LLMがコード提案を生成                                    │
│  4. 提案がユーザーに表示                                     │
│  5. プロンプトと提案は【破棄】                               │
└─────────────────────────────────────────────────────────────┘
```

**データ保持に関する重要なポイント**:
- プロンプト（コードやコンテキスト）は**保持されない**
- 提案が返された後、データは**破棄**される
- 基盤モデルのトレーニングには**使用されない**

#### Copilot Chatのデータ処理

Copilot Chatは、コード補完とは異なるデータ処理を行います：

| 項目 | コード補完 | Copilot Chat |
|------|-----------|--------------|
| データ保持 | なし | 最大28日間 |
| 会話履歴 | なし | 維持される |
| コンテキスト理解 | 現在のファイルのみ | 会話全体 |
| ユースケース | リアルタイムコード提案 | 対話的なコーディング支援 |

**Copilot Chatの特徴**:
- プロンプト、提案、関連コンテキストを**28日間保持**
- 会話履歴を維持してコンテキスト理解を向上
- フォーマット処理によりチャットインターフェースに最適化

#### Copilot Chatがサポートするプロンプトタイプ

| プロンプトタイプ | 例 |
|-----------------|-----|
| **直接的な質問** | 「Pythonでクイックソートを実装するには？」 |
| **コード関連のリクエスト** | 「階乗を計算する関数を書いて」 |
| **オープンエンドなクエリ** | 「クリーンコードのベストプラクティスは？」 |
| **コンテキストプロンプト** | 「このコードを改善できる？」 |

#### プラン別のデータ処理比較

| 機能 | Individual | Business | Enterprise |
|------|-----------|----------|------------|
| プロンプト保持 | オプトアウト可能 | 保持なし | 保持なし |
| モデルトレーニングへの使用 | オプトアウト可能 | なし | なし |
| IP補償 | なし | あり | あり |
| 組織管理 | なし | あり | あり |

#### Individual契約者のオプトアウト設定

GitHub Copilot Individualユーザーは、プロンプト共有をオプトアウトできます：

1. GitHubの **Settings** に移動
2. **Copilot** セクションを選択
3. **Allow GitHub to use my code snippets from the code editor for product improvements** のチェックを外す

> **注意**: Business/Enterpriseプランでは、ユーザーのコードはモデルのトレーニングに使用されず、プロンプトも保持されません。

---

## レクチャー7: LLMの制限（コンテキストウィンドウ・データの古さ）
**時間目安: 10分**

### 学習目標
- LLMのコンテキストウィンドウの制限を理解する
- トレーニングデータの古さの問題を説明できる
- 制限を考慮した効果的な活用方法を把握する

### 内容

#### コンテキストウィンドウの制限

LLMには一度に処理できる情報量に**制限**があります：

```
┌────────────────────────────────────────────────────────────┐
│                  コンテキストウィンドウ                     │
├────────────────────────────────────────────────────────────┤
│  ┌──────────────────────────────────────────────────────┐  │
│  │     処理可能な範囲（200-500行 / 数千トークン）         │  │
│  │  ┌──────────────────────────────────────────────┐    │  │
│  │  │  Copilot Chat: 4,000トークン                 │    │  │
│  │  └──────────────────────────────────────────────┘    │  │
│  └──────────────────────────────────────────────────────┘  │
│                                                            │
│  ← この範囲外のコードは考慮されない →                       │
└────────────────────────────────────────────────────────────┘
```

| 機能 | コンテキストウィンドウ |
|------|---------------------|
| **コード補完** | 約200-500行 / 数千トークン |
| **Copilot Chat** | 4,000トークン |

**トークンとは**:
- テキストの単位（単語、サブワード、文字など）
- 英語では1単語 ≒ 1-2トークン
- コードでは記号や括弧も1トークンとしてカウント

#### トレーニングデータの古さ

LLMは過去のデータでトレーニングされているため、**最新の情報を知らない**場合があります：

| 制限 | 影響 |
|------|------|
| **カットオフ日** | トレーニングデータには終了日がある |
| **新しいライブラリ** | 最新のAPIやライブラリを知らない可能性 |
| **フレームワーク更新** | 最新の構文やベストプラクティスに対応していない可能性 |
| **新しい言語機能** | 言語の最新機能を提案できない可能性 |

#### 非決定論的な出力

LLMの出力には以下の特性があります：

```
同じ入力 → 異なる出力の可能性
   │
   ├── 出力A: function add(a, b) { return a + b; }
   ├── 出力B: const add = (a, b) => a + b;
   └── 出力C: function sum(a, b) { return a + b; }
```

**非決定論的出力の特徴**:
- 同じプロンプトでも**異なる結果**が返される可能性
- 確率的なトークン選択に基づく
- 許容範囲の出力であれば問題にならないことが多い

#### LLMの制限に関する重要な考慮事項

| 考慮事項 | 説明 |
|---------|------|
| **1. 非決定論的** | 同じ入力で異なる出力が得られる可能性 |
| **2. 品質のばらつき** | 低品質または不正確な出力が生成される可能性 |
| **3. コンテキストの制限** | ユーザーの意図や全体的なコンテキストを完全に理解できない |
| **4. トレーニングデータの制限** | 最新の機能やパターンに対応していない可能性 |

#### 効果的な活用のためのヒント

**1. 複雑な問題を分割する**
```
❌ 悪い例:
「eコマースサイト全体のバックエンドを作って」

✅ 良い例:
「ユーザー認証のためのログイン関数を作って」
「商品検索のAPIエンドポイントを作って」
```

**2. 関連するコードスニペットを提供する**
```python
# コンテキストを明確にする
# 既存のUserクラスを参照して、新しいメソッドを追加
class User:
    def __init__(self, name, email):
        self.name = name
        self.email = email

    # ここに新しいメソッドを追加
    # validate_emailメソッドを作成
```

**3. コンテキストウィンドウを意識する**
- 大きなファイルよりも**関連する部分**を開く
- 必要な情報を**集中的に**提供
- 不要なタブは閉じて**ノイズを減らす**

> **まとめ**: LLMの制限を理解し、それに合わせたプロンプト作成を行うことで、より効果的にGitHub Copilotを活用できます。

---

## 確認テスト: GitHub Copilotのしくみとデータ処理

### 問題1
GitHub Copilotがコード補完候補を生成するために主に使用しているものは何ですか？

- A) ルールベースのパターンマッチング
- B) 大規模言語モデル（LLM）
- C) データベースクエリ
- D) 静的コード解析のみ

<details>
<summary>解答</summary>

**正解: B) 大規模言語モデル（LLM）**

GitHub Copilotは大規模言語モデル（LLM）を使用してコード補完候補を生成します。LLMは膨大なコードデータでトレーニングされており、コンテキストを理解して適切なコード提案を行います。

</details>

---

### 問題2
GitHub Copilotの処理プロセスに含まれないステップはどれですか？

- A) コンテキストの収集とプロンプトの構築
- B) LLMによる提案の生成
- C) ユーザーのコードをトレーニングデータとして保存
- D) フィルター処理と後処理

<details>
<summary>解答</summary>

**正解: C) ユーザーのコードをトレーニングデータとして保存**

GitHub Copilotの処理プロセスには、コンテキスト収集、プロンプト構築、LLMによる提案生成、フィルター処理が含まれます。ただし、GitHub Copilot for Businessでは、ユーザーのコードをAIモデルのトレーニングデータとして保存または使用しません。

</details>

---

### 問題3
Fill-in-the-Middle（FIM）手法について正しい説明はどれですか？

- A) ファイルの最初の部分のみを読み取る
- B) カーソルの前後のコンテキストを使用して、より適切な提案を生成する
- C) 外部ファイルのみを参照する
- D) コードを削除する技術

<details>
<summary>解答</summary>

**正解: B) カーソルの前後のコンテキストを使用して、より適切な提案を生成する**

Fill-in-the-Middle（FIM）手法では、カーソルの前のコード（プレフィックス）と後のコード（サフィックス）の両方をコンテキストとして使用します。これにより、既存のコード構造に適合する、より正確で関連性の高い提案が可能になります。

</details>

---

### 問題4
GitHub Copilotが提案を生成した後に行われる処理として正しいものはどれですか？

- A) 提案をそのまま表示する
- B) 一致コードの識別、フィルタリング、ユーザーへの表示
- C) 提案を永続的に保存する
- D) 自動的にコードをコミットする

<details>
<summary>解答</summary>

**正解: B) 一致コードの識別、フィルタリング、ユーザーへの表示**

LLMが提案を生成した後、GitHub Copilotは一致コードの識別（パブリックコードとの照合）、不適切なコンテンツのフィルタリング、そしてユーザーへの提案表示という処理を行います。

</details>

---

### 問題5
LLMのコンテキストウィンドウに関する制限として正しいものはどれですか？

- A) 制限なく無限のコードを処理できる
- B) 一度に処理できるトークン数に制限があり、大きなファイルすべてを理解することは困難
- C) テキストのみを処理でき、コードは処理できない
- D) 英語のみを処理できる

<details>
<summary>解答</summary>

**正解: B) 一度に処理できるトークン数に制限があり、大きなファイルすべてを理解することは困難**

LLMにはコンテキストウィンドウという制限があり、一度に処理できるトークン（単語やコードの断片）の数に上限があります。そのため、非常に大きなファイルやプロジェクト全体のすべてのコンテキストを一度に理解することは困難です。

</details>

---

### 問題6
GitHub Copilotのプロキシサービスの役割として正しくないものはどれですか？

- A) ユーザーからのリクエストを受け取る
- B) LLMへのリクエストを転送する
- C) ユーザーのコードを永続的に保存する
- D) フィルタリングと後処理を行う

<details>
<summary>解答</summary>

**正解: C) ユーザーのコードを永続的に保存する**

GitHub Copilotのプロキシサービスは、ユーザーからのリクエスト受信、LLMへの転送、レスポンスのフィルタリングと後処理を担当します。ただし、ユーザーのコードを永続的に保存することはありません（特にBusinessプランでは、プロンプトや提案は保持されません）。

</details>

---

### 問題7
GitHub Copilot for BusinessおよびEnterpriseプランで提供される知的財産（IP）補償について正しいものはどれですか？

- A) すべての設定で自動的に適用される
- B) Matching public codeをBlockに設定した場合にのみ適用される
- C) Individualプランでも利用可能である
- D) IP補償は提供されていない

<details>
<summary>解答</summary>

**正解: B) Matching public codeをBlockに設定した場合にのみ適用される**

IP補償を受けるためには、「Matching public code」設定を「Block」に設定する必要があります。この設定により、パブリックコードと一致する提案がブロックされ、GitHubが知的財産権に関する法的責任を負う保護が適用されます。

</details>

---

### 問題8
Copilot Chatのデータ保持期間として正しいものはどれですか？

- A) データは保持されない
- B) 24時間
- C) 28日間
- D) 永続的に保持

<details>
<summary>解答</summary>

**正解: C) 28日間**

Copilot Chatでは、プロンプト、提案、および関連するコンテキストが最大28日間保持されます。これは、会話履歴を維持してコンテキスト理解を向上させるためです。一方、コード補完機能ではデータは保持されません。

</details>

---

### 問題9
GitHub Copilotが収集するコンテキスト情報に含まれないものはどれですか？

- A) カーソル前後のコード
- B) ファイル名とファイルタイプ
- C) ユーザーの個人的なブラウザ履歴
- D) 隣接する開いているタブの情報

<details>
<summary>解答</summary>

**正解: C) ユーザーの個人的なブラウザ履歴**

GitHub Copilotは、カーソル前後のコード、ファイル名とタイプ、隣接する開いているタブ、プロジェクト構造などの情報を収集します。しかし、ユーザーの個人的なブラウザ履歴などのIDE外の情報は収集しません。

</details>

---

### 問題10
LoRA（Low-Rank Adaptation）ファインチューニングの利点として正しいものはどれですか？

- A) すべてのパラメータを再トレーニングする
- B) リソース消費が多く、処理時間が長い
- C) オリジナルモデルを変更せず、効率的にファインチューニングできる
- D) モデルの性能が大幅に低下する

<details>
<summary>解答</summary>

**正解: C) オリジナルモデルを変更せず、効率的にファインチューニングできる**

LoRAファインチューニングは、オリジナルモデルを変更せずに各レイヤーに小さなトレーニング可能なパラメータを追加する手法です。これにより、時間とリソースを大幅に節約しながら、従来のファインチューニングと同等以上の性能を達成できます。

</details>
